{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Membrain-Seg","text":"<p>Membrain-Seg is a Python project developed by teamtomo for membrane segmentation in 3D for cryo-electron tomography (cryo-ET). This tool aims to provide researchers with an efficient and reliable method for segmenting membranes in 3D microscopic images. Membrain-Seg is currently under early development, so we may make breaking changes between releases.</p> <p> </p>"},{"location":"#overview","title":"Overview","text":"<p>MemBrain-seg is a practical tool for membrane segmentation in cryo-electron tomograms. It's built on the U-Net architecture and makes use of a pre-trained model for efficient performance. The U-Net architecture and training parameters are largely inspired by nnUNet<sup>1</sup>.</p> <p>If you wish, you can also train a new model using your own data, or combine it with our available public dataset. (soon to come!)</p> <p>To enhance segmentation, MemBrain-seg includes preprocessing functions. These help to adjust your tomograms so they're similar to the data our network was trained on, making the process smoother and more efficient.</p> <p>Explore MemBrain-seg, use it for your needs, and let us know how it works for you!</p> <pre><code>[1] Isensee, F., Jaeger, P. F., Kohl, S. A., Petersen, J., &amp; Maier-Hein, K. H. (2020). nnU-Net: a self-configuring method \nfor deep learning-based biomedical image segmentation. Nature Methods, 1-9.\n</code></pre>"},{"location":"#installation","title":"Installation","text":"<p>For detailed installation instructions, please look here.</p>"},{"location":"#features","title":"Features","text":""},{"location":"#segmentation","title":"Segmentation","text":"<p>Segmenting the membranes in your tomograms is the main feature of this repository.  Please find more detailed instructions here.</p>"},{"location":"#preprocessing","title":"Preprocessing","text":"<p>Currently, we provide the following two preprocessing options: - pixel size matching: Rescale your tomogram to match the training pixel sizes - Fourier amplitude matching: Scale Fourier components to match the \"style\" of different tomograms</p> <p>For more information, see the Preprocessing subsection.</p>"},{"location":"#model-training","title":"Model training","text":"<p>It is also possible to use this package to train your own model. Instructions can be found here.</p>"},{"location":"#patch-annotations","title":"Patch annotations","text":"<p>In case you would like to train a model that works better for your tomograms, it may be beneficial to add some more patches from your tomograms to the training dataset.  Recommendations on how to to this can be found here.</p>"},{"location":"installation/","title":"Installation","text":"<p>These installation instructions are very preliminary, and surely will not work on all systems. But if any problems come up, do not hesitate to contact us (lorenz.lamm@helmholtz-munich.de).</p>"},{"location":"installation/#step-1-clone-repository","title":"Step 1: Clone repository","text":"<p>Make sure to have git installed, then run <pre><code>git clone https://github.com/teamtomo/membrain-seg.git\n</code></pre></p>"},{"location":"installation/#step-2-create-a-virtual-environment","title":"Step 2: Create a virtual environment","text":"<p>Before running any scripts, you should create a virtual Python environment. In these instructions, we use Miniconda for managing your virtual environments, but any alternative like Conda, Mamba, virtualenv, venv, ... should be fine.</p> <p>If you don't have any, you could install Miniconda from the official website.</p> <p>Now you can create a new virtual environment using <pre><code>conda create --name &lt;env_name&gt; python=3.9\n</code></pre></p> <p>In order to use it, you need to activate the environment: <pre><code>conda activate &lt;env_name&gt;\n</code></pre></p>"},{"location":"installation/#step-3-install-membrain-seg-and-its-dependencies","title":"Step 3: Install MemBrain-seg and its dependencies","text":"<p>Move to the folder \"membrain-seg\" (from the cloned repository in Step 1) that contains the \"src\" folder. Here, run</p> <pre><code>cd membrain-seg\npip install .\n</code></pre> <p>This will install MemBrain-seg and all dependencies required for segmenting your tomograms.</p>"},{"location":"installation/#step-4-validate-installation","title":"Step 4: Validate installation","text":"<p>As a first check whether the installation was successful, you can run <pre><code>membrain\n</code></pre> This should display the different options you can choose from MemBrain, like \"segment\" and \"train\", similar to the screenshot below:</p> <p> </p>"},{"location":"installation/#step-5-download-pre-trained-segmentation-model-optional","title":"Step 5: Download pre-trained segmentation model (optional)","text":"<p>We recommend to use denoised (ideally Cryo-CARE<sup>1</sup>) tomograms for segmentation. However, our current best model is available for download here and should also work on non-denoised data. Please let us know how it works for you.</p> <p>NOTE: Previous model files are not compatible with MONAI v1.3.0 or higher. So if you're using v1.3.0 or higher, consider downgrading to MONAI v1.2.0 or downloading this adapted version of our most recent model file. </p> <p>If the given model does not work properly, you may want to try one of our previous versions:</p> <p>Other (older) model versions: - v9 -- best model until 10th Aug 2023 - v9b -- model for non-denoised data until 10th Aug 2023</p> <p>Once downloaded, you can use it in MemBrain-seg's Segmentation functionality to segment your tomograms.</p> <pre><code>[1] T. -O. Buchholz, M. Jordan, G. Pigino and F. Jug, \"Cryo-CARE: Content-Aware Image Restoration for Cryo-Transmission Electron Microscopy Data,\" 2019 IEEE 16th International Symposium on Biomedical Imaging (ISBI 2019), Venice, Italy, 2019, pp. 502-506, doi: 10.1109/ISBI.2019.8759519.\n</code></pre>"},{"location":"installation/#troubleshooting","title":"Troubleshooting","text":"<p>Here is a collection of common issues and how to fix them:</p> <ul> <li><code>RuntimeError: The NVIDIA driver on your system is too old (found version 11070). Please update your GPU driver by downloading and installing a new version from the URL: http://www.nvidia.com/Download/index.aspx Alternatively, go to: https://pytorch.org to install a PyTorch version that has  been compiled with your version of the CUDA driver.</code> </li> </ul> <p>The latest Pytorch versions require higher CUDA versions that may not be installed on your system yet. You can either install the new CUDA version or (maybe easier) downgrade Pytorch to a version that is compatible:</p> <p><code>pip uninstall torch</code></p> <p><code>pip install torch==2.0.1</code></p>"},{"location":"Usage/Annotations/","title":"How to create training annotations from your own dataset?","text":"<p>This is an example guide on how to create training data in order to improve MemBrain-seg's performance on your tomograms.  The annotation strategy was developed as part of a Helmholtz Imaging collaboration with Fabian Isensee &amp; Sebastian Ziegler.</p> <p>In parallel to this guide, please also check Simon Zufferey's YouTube tutorial accompanying the instructions on this size. </p> <p>Important note: While this guide describes how to improve the performance for your own tomograms, we highly encourage you to also share your generated training patches with us and the community, so that everybody can benefit from a more generalized performance of the model. We are currently working on a platform to easily share them. In the meantime, please reach out to us (Lorenz.Lamm@helmholtz-munich.de) to discuss how to best share your patches without giving away too much of your own data.</p>"},{"location":"Usage/Annotations/#general-idea","title":"General idea","text":"<p>Nobody would like (or has the time) to segment membranes a whole tomogram manually from scratch. Therefore, our approach is to extract small patches (160^3) from the tomogram an create manual annotations for these.</p> <p>For this, we do not start from scratch, but use the prediction of the currently best MemBrain-seg model. Ideally, this will already segment most areas well and minimize the workload for corrections.</p>"},{"location":"Usage/Annotations/#workflow","title":"Workflow","text":"<p>The steps described in this tutorial are:</p> <ol> <li>Which software to use</li> <li>Extraction of patches for correction</li> <li>Performing the corrections</li> <li>Merging the corrections</li> </ol>"},{"location":"Usage/Annotations/#software","title":"Software","text":"<p>You will need software to inspect your tomograms and MemBrain-seg's segmentations, as well as to perform the corrections. For both of these tasks, we use MITK Workbench, but any software with these functionalities will do, e.g. Amira or Napari.</p> <p>In our YouTube playlist, you can also find a video on basic usage of MITK for patch correction.</p>"},{"location":"Usage/Annotations/#patch-extraction","title":"Patch extraction","text":"<p>In order to not have to correct the entire tomogram, we focus on small patches (160^3) where the segmentation performance is particularly bad. We crop these patches out of the tomogram and correct them manually.</p> <p>In order to extract the patches from the tomogram, you can open the tomograms together with MemBrain's predicted segmentation, e.g. in MITK or IMOD. Now, you find regions where MemBrain's performance is not satisfying, but you can still see whether there should be a membrane or not. Use the center coordinates of these areas to extract patches (patches will be extracted centered around the x-, y-, and z-cooordintes given).</p> <p> </p> <p>Once you found all patches that you would like to extract (we recommend around 2-5 per tomogram), you can extract them using the following script</p> <p><pre><code>patch_corrections extract_patches\n</code></pre> Running this command will open the help of this function and guide you through the required parameters.</p> <p>Simon also describes this process of patch selection in his first episode on YouTube.</p>"},{"location":"Usage/Annotations/#corrections","title":"Corrections","text":"<p>The goal of the corrections is to assign every voxel in your extracted patch with the correct label (i.e. \"membrane\" or \"no membrane\"). However, each tomogram will probably have regions where it is very hard to tell where exactly the membrane is or if there is a membrane at all. In these cases, we want to use the \"ignore\" label. This label will not influence training of the U-Net in any direction, so whenever you are in doubt, it's best to assign the \"ignore\" label. All voxels not assigned to the ignore label will contribute to the network training and should therefore be very reliable!</p>"},{"location":"Usage/Annotations/#correction-workflow","title":"Correction workflow","text":"<p>The starting point for the generation of new training patches is the segmentation produced by the previous best MemBrain-seg segmentation. In order to use this segmentation for re-training the network, we need to make the annotations as good as we can. We do this by modifying the segmentation by creating different segmentation classes:</p> <ol> <li>Starting point: MemBrain-seg segmentation</li> <li>Subtract all \"remove\" annotations from the segmentation</li> <li>Add all \"add\" annotations to the segmentation</li> <li>Assign defined \"ignore\" labels</li> </ol> <p>Note: Steps 1 to 4 are performed in the background when you merge your corrections with the command  <pre><code>patch_corrections merge_corrections\n</code></pre></p>"},{"location":"Usage/Annotations/#folder-structure","title":"Folder structure","text":"<p>During the correction of the patches, you will generate different files (\"add\", \"remove\", and \"ignore\" labels). In order for them to be merged properly with the original segmentation, you should follow the following folder structure:</p> <pre><code>root_directory/\n    \u251c\u2500\u2500 labels_dir/\n    \u2502   \u251c\u2500\u2500 label_file1\n    \u2502   \u251c\u2500\u2500 label_file2\n    \u2502   \u251c\u2500\u2500 label_file3\n    \u2502   \u2514\u2500\u2500 ...\n    \u251c\u2500\u2500 corrections_dir/\n    \u2502   \u251c\u2500\u2500 label_file1/\n    \u2502   \u2502   \u251c\u2500\u2500 Add1.nrrd\n    \u2502   \u2502   \u251c\u2500\u2500 Add2.nrrd\n    \u2502   \u2502   \u251c\u2500\u2500 Remove1.nrrd\n    \u2502   \u2502   \u251c\u2500\u2500 Ignore1.nrrd\n    \u2502   \u2502   \u251c\u2500\u2500 Ignore2.nrrd\n    \u2502   \u2502   \u251c\u2500\u2500 Ignore3.nrrd\n    \u2502   \u2502   \u2514\u2500\u2500 ...\n    \u2502   \u251c\u2500\u2500 label_file2/\n    \u2502   \u2502   \u251c\u2500\u2500 Add1.nrrd\n    \u2502   \u2502   \u251c\u2500\u2500 Add2.nrrd\n    \u2502   \u2502   \u251c\u2500\u2500 Ignore1.nrrd\n    \u2502   \u2502   \u2514\u2500\u2500 ...\n    \u2502   \u251c\u2500\u2500 label_file3/\n    \u2502   \u2502   \u251c\u2500\u2500 Add1.nrrd\n    \u2502   \u2502   \u251c\u2500\u2500 Ignore1.nrrd\n    \u2502   \u2502   \u2514\u2500\u2500 ...\n    \u2502   \u2514\u2500\u2500 ...\n    \u2514\u2500\u2500 out_dir/ (This directory will be filled with the corrected files\n</code></pre>"},{"location":"Usage/Annotations/#remove-label","title":"Remove label","text":"<p>Whenever you find regions in your tomogram that MemBrain-seg segmented falsely (i.e., they actually are not a membrane), you should correct for this using the \"remove\" annotations.</p> <p>For this, you create a segmentation layer in MITK and brush over all regions that are falsely annotated. (Can be coarse if no neighboring voxels belong to a membrane). Then, you can save the resulting segmentation as \"Remove1.nrrd\" (or replace 1 with the current number of your segmentation).</p> <p> </p> <p>Setting the \"Remove\" label is also explained in Simon's YouTube video about the remove label.</p>"},{"location":"Usage/Annotations/#add-label","title":"Add label","text":"<p>For the \"add\" annotations, you look for areas in your patch where MemBrain-seg did not segment a membrane, even though the membrane is clearly visible. In these regions, you can now accurately delineate where the membrane is, i.e. you assign all voxels belonging to a membrane to the \"add\" annotation.</p> <p>Similarly to the \"remove\" label, you should save the resulting segmentation as \"Add1.nrrd\".</p> <p> </p> <p>You can find visualizations of the \"Add label\" in this YouTube video.</p>"},{"location":"Usage/Annotations/#ignore-label","title":"Ignore label","text":"<p>The \"ignore\" annotation is used whenever you are not sure where exactly the membrane is or whether there is a membrane at all. In these cases, you can coarsely annotate these difficult regions. Thereby, you don't need to be very accurate and can coarsely capture the area.</p> <p>Similarly to the \"remove\" and \"add\" label, you should save the resulting segmentation as \"Ignore1.nrrd\".</p> <p>In the example below, one can see that the membrane should be closing somewhere, but it is not possible to exactly delineate where the membrane is going through. In these cases, it is best to assign the \"ignore\" label (purple)</p> <p> </p> <p>More details and examples of the \"Ignore label\" can be found in this clip.</p>"},{"location":"Usage/Annotations/#merging-of-corrections","title":"Merging of corrections","text":"<p>After saving all your files with appropriate naming, you should check again that your saved corrections follow the folder structure described above.</p> <p>Then, you can merge your corrections into training patches that can be used for re-training:</p> <pre><code>patch_corrections merge_corrections --labels-dir &lt;path-to-your-labels-dir&gt; --corrections-dir &lt;path-to-your-corrections&gt; --out-dir &lt;out-directory&gt;\n</code></pre> <p>Hereby,  - \"path-to-your-labels-dir\" should be replaced with the folder that contains the labels of your extracted patches (\"labels_dir\" in above folder structure) - \"path-to-your-corrections\" should be the folder containing all sub-folders for all patch corrections (\"corrections_dir\" in above folder structure) - \"out-directory\" should be the folder where the merged corrections should be stored</p>"},{"location":"Usage/Annotations/#and-now","title":"And now?","text":"<p>Unfortunately, we do not publicly provide our full training dataset yet, as it is still under development. But that should not stop you from having a model that works well on your tomograms. Do not hesitate to reach out (Lorenz.Lamm@helmholtz-munich.de) and we will find a solution!</p>"},{"location":"Usage/Preprocessing/","title":"Preprocessing","text":""},{"location":"Usage/Preprocessing/#introduction","title":"Introduction","text":"<p>This is a quick guide on how to use the tomo_preprocessing module for MemBrain-seg. </p> <p>In order to improve the segmentation performance of MemBrain-seg on your tomograms, it may be beneficial to first perform some preprocessing to \"normalize\" the tomograms to similar styles as the training data.</p> <p>This module currently allows you to use the following preprocessing methods:</p> <ul> <li>Pixel size matching: Rescaling of your tomogram to a similar pixel size as the training data</li> <li>Fourier amplitude matching: Rescaling of Fourier components to pronounce different features in the tomograms (adapted from DeePiCt)</li> </ul>"},{"location":"Usage/Preprocessing/#table-of-contents","title":"Table of Contents","text":"<ul> <li>When to use what?</li> <li>Usage</li> <li>Available Commands</li> <li>Pixel Size Matching</li> <li>Fourier Amplitude Matching</li> </ul>"},{"location":"Usage/Preprocessing/#when-to-use-what","title":"When to use what?","text":"<p>We are still exploring when it makes sense to use which preprocessing technique. But here are  already some rules of thumb:</p> <ol> <li>Whenever your pixel sizes differs by a lot from around 10-12\u00c5 / pixel, you should consider using pixel size matching. We recommend to match to a pixel size of 10\u00c5.</li> <li>The Fourier amplitude matching only works in some cases, depending on the CTFs of input  and target tomograms. Our current recommendation is: If you're not satisfied with MemBrain's  segmentation performance, why not give the amplitude matching a shot?</li> </ol> <p>More detailed guidelines are in progress!</p>"},{"location":"Usage/Preprocessing/#usage","title":"Usage","text":"<p>You can control all commands of this preprocessing module by typing <code>tomo_preprocessing</code>+ some options. To view all available commands, use:</p> <pre><code>tomo_preprocessing --help\n</code></pre> <p>For help on a specific command, use:</p> <pre><code>tomo_preprocessing &lt;command&gt; --help\n</code></pre>"},{"location":"Usage/Preprocessing/#available-commands","title":"Available Commands","text":"<ul> <li>match_pixel_size: Tomogram rescaling to specified pixel size. Example: <code>tomo_preprocessing match_pixel_size --input-tomogram &lt;path-to-tomo&gt; --output-path &lt;path-to-output&gt; --pixel-size-out 10.0 --pixel-size-in &lt;your-px-size&gt;</code></li> <li>match_seg_to_tomo: Segmentation rescaling to fit to target tomogram's shape. Example: <code>tomo_preprocessing match_seg_to_tomo --seg-path &lt;path-to-seg&gt; --orig-tomo-path &lt;path-to-tomo&gt; --output-path &lt;path-to-output&gt;</code></li> <li>extract_spectrum: Extracts the radially averaged amplitude spectrum from the input tomogram. Example: <code>tomo_preprocessing extract_spectrum --input-path &lt;path-to-tomo&gt; --output-path &lt;path-to-output&gt;</code></li> <li>match_spectrum: Match amplitude of Fourier spectrum from input tomogram to target spectrum. Example: <code>tomo_preprocessing match_spectrum --input &lt;path-to-tomo&gt; --target &lt;path-to-spectrum&gt; --output &lt;path-to-output&gt;</code></li> </ul>"},{"location":"Usage/Preprocessing/#pixel-size-matching","title":"Pixel Size Matching","text":"<p>Pixel size matching is recommended when your tomogram pixel sizes differs strongly from the training pixel size range (roughly 10-14\u00c5). You can perform it using the command</p> <pre><code>tomo_preprocessing match_pixel_size --input-tomogram &lt;path-to-tomo&gt; --output-path &lt;path-to-output&gt; --pixel-size-out 10.0 --pixel-size-in &lt;your-px-size&gt;\n</code></pre> <p>after adjusting the paths to your respective tomograms. Afterwards, you can perform MemBrain's segmentation on the rescaled tomogram (i.e. the one specified in <code>--output-path</code>). Now, this new segmentation does not have the same shape as the original non-pixel-size-matched tomogram. To rescale the new segmentation to the original tomogram again, you can use</p> <pre><code>tomo_preprocessing match_seg_to_tomo --seg_path &lt;path-to-seg&gt; --orig-tomo-path &lt;path-to-tomo&gt; --output-path &lt;path-to-output&gt;\n</code></pre> <p>where the <code>--seg-path</code>is the segmentation created by MemBrain and the <code>--orig-tomo-path</code>is the original tomogram before rescaling to the new pixel size. The output of this function will be MemBrain's segmentation, but matched to the pixel size of the original tomogram.</p>"},{"location":"Usage/Preprocessing/#fourier-amplitude-matching","title":"Fourier Amplitude Matching","text":"<p>Fourier amplitude matching is performed in two steps:</p> <ol> <li>Extraction of the target Fourier spectrum: <pre><code>tomo_preprocessing extract_spectrum --input-path &lt;path-to-tomo&gt; --output-path &lt;path-to-output&gt;\n</code></pre> This extracts the radially averaged Fourier spectrum and stores it into a .tsv file.</li> <li>Matching of the input tomogram to the extracted spectrum: <pre><code>tomo_preprocessing match_spectrum --input &lt;path-to-tomo&gt; --target &lt;path-to-spectrum&gt; --output &lt;path-to-output&gt;\n</code></pre> Now, the input tomograms Fourier components are re-scaled based on the equalization kernel computed from the input tomogram's radially averaged Fourier intensities, and the previously extracted .tsv file.</li> </ol>"},{"location":"Usage/Segmentation/","title":"Segmentation","text":"<p>Welcome to the central feature of MemBrain-seg: The segmentation of your tomograms!</p> <p>This guide provides detailed instructions to perform segmentation on your tomograms using a pre-trained model.</p>"},{"location":"Usage/Segmentation/#membrain-seg-workflow","title":"MemBrain-seg Workflow","text":"<ol> <li>(Optional) Preprocessing<ul> <li>pixel size matching</li> <li>Fourier amplitude matching</li> </ul> </li> <li>Predict segmentation</li> <li>(Optional) Match pixel size of output segmentation</li> </ol> <p>For the optional preprocessing steps, find more information here</p>"},{"location":"Usage/Segmentation/#preparations","title":"Preparations","text":"<p>For the prediction, you will basically need two files:</p> <ol> <li>The tomogram you would like to segment. It may make sense to use a preprocessed tomogram.</li> <li>A pre-trained MemBrain segmentation model</li> </ol> <p>We recommend to use denoised (ideally Cryo-CARE<sup>1</sup>) tomograms for segmentation. However, our current best model is available for download here and should also work on non-denoised data. Please let us know how it works for you. If the given model does not work properly, you may want to try one of our previous versions:</p> <p>Other (older) model versions: - v9 -- best model until 10th Aug 2023 - v9b -- model for non-denoised data until 10th Aug 2023</p> <p>Please note that our best model changes often, as we are still in the development phase. So you can check in from time to time and see whether the model improved. If you have problems with the model, please write an email to lorenz.lamm@helmholtz-munich.de</p> <pre><code>[1] T. -O. Buchholz, M. Jordan, G. Pigino and F. Jug, \"Cryo-CARE: Content-Aware Image Restoration for Cryo-Transmission Electron Microscopy Data,\" 2019 IEEE 16th International Symposium on Biomedical Imaging (ISBI 2019), Venice, Italy, 2019, pp. 502-506, doi: 10.1109/ISBI.2019.8759519.\n</code></pre>"},{"location":"Usage/Segmentation/#prediction","title":"Prediction","text":"<p>Typing <pre><code>membrain segment\n</code></pre> will display the segmentation command line interface and show available options.</p> <p> </p> <p>For example, for the prediction, you only need to type</p> <pre><code>membrain segment --tomogram-path &lt;path-to-your-tomo&gt; --ckpt-path &lt;path-to-your-model&gt;\n</code></pre> <p>Running this will segment your tomogram, and store the resulting .mrc file into the ./predictions  folder. If you would like to change this folder, you can simply specify another folder using the <code>--out_folder</code> argument:</p> <pre><code>membrain segment --tomogram-path &lt;path-to-your-tomo&gt; --ckpt-path &lt;path-to-your-model&gt; --out-folder &lt;your-preferred-folder&gt;\n</code></pre> <p>It is now also possible to assign different labels to different membrane instances via computing connected components and also remove small connected components: <pre><code>membrain segment --tomogram-path &lt;path-to-your-tomo&gt; --ckpt-path &lt;path-to-your-model&gt; --store-connected-components\n</code></pre></p> <p>You can also compute the connected components after you have segmented your tomogram.</p>"},{"location":"Usage/Segmentation/#more-membrain-segment-arguments","title":"more membrain segment arguments:","text":"<p>--tomogram-path: TEXT Path to the tomogram to be segmented [default: None] </p> <p>--ckpt-path TEXT Path to the pre-trained model checkpoint that should be used. [default: None] </p> <p>--out-folder TEXT Path to the folder where segmentations should be stored. [default: ./predictions]</p> <p>--store-probabilities / --no-store-probabilities: Should probability maps be output in addition to segmentations? [default: no-store-probabilities]</p> <p>--store-connected-components / no-store-connected-components: Should connected components of the segmentation be computed? [default: no-store-connected-components]  </p> <p>--connected-component-thres: Threshold for connected components. Components smaller than this will be removed from the segmentation. [default: None]</p> <p>--test-time-augmentation / --no-test-time-augmentation: Should 8-fold test time augmentation be used? If activated (default), segmentations tendo be slightly better, but runtime is increased.</p> <p>--segmentation-threshold: Set a custom threshold for thresholding your membrane scoremap to increase / decrease segmented membranes (default: 0.0).</p> <p>--sliding-window-size INTEGER Sliding window size used for inference. Smaller values than 160 consume less GPU, but also lead to worse segmentation results! [default: 160] </p> <p>--help Show this message and exit.     </p>"},{"location":"Usage/Segmentation/#note","title":"Note:","text":"<p>MemBrain-seg automatically detects a CUDA-enabled GPU, if available, and will execute the segmentation on it. Using a GPU device is highly recommended to accelerate the segmentation process.</p>"},{"location":"Usage/Segmentation/#note2","title":"Note#2:","text":"<p>Running MemBrain-seg on a GPU requires at least roughly 8GB of GPU space.</p>"},{"location":"Usage/Segmentation/#emergency-tip","title":"Emergency tip:","text":"<p>In case you don't have enough GPU space, you can also try adjusting the <code>--sliding-window-size</code> parameter. By default, it is set to 160. Smaller values will require less GPU space, but also lead to worse segmentation results!</p>"},{"location":"Usage/Segmentation/#connected-components","title":"Connected components","text":"<p>If you have segmented your tomograms already, but would still like to extract the connected components of the segmentation, you don't need to re-do the segmentation, but can simply use the following command: <pre><code>membrain components --segmentation-path &lt;path-to-your-segmentation&gt; --connected-component-thres 50 --out-folder &lt;folder-to-store-components&gt;\n</code></pre></p>"},{"location":"Usage/Segmentation/#note_1","title":"Note:","text":"<p>Computing the connected components, and particularly also removing the small components can be quite compute intensive and take a while.</p>"},{"location":"Usage/Segmentation/#custom-thresholding","title":"Custom thresholding","text":"<p>In some cases, the standard threshold (\\(0.0\\)) may not be the ideal value for segmenting your tomograms. In order to explore what threshold may be best, you can use the above segmentation command with the flag <code>--store-probabilities</code>. This will store a membrane scoremap that you can threshold using different values using the command:</p> <p><pre><code>membrain thresholds --scoremap-path &lt;path-to-scoremap&gt;\n        --thresholds -1.5 --thresholds -0.5 --thresholds 0.0 --thresholds 0.5\n</code></pre> In this way, you can pass as many thresholds as you would like and the function will output one segmentation for each.</p>"},{"location":"Usage/Segmentation/#post-processing","title":"Post-Processing","text":"<p>If you have pre-processed your tomogram using pixel size matching, you may want to rescale your  segmentation back to the shape of the original tomogram.</p>"},{"location":"Usage/Training/","title":"Training","text":"<p>MemBrain-seg is designed to work out-of-the-box and ideally will not require training your own model.</p> <p>However, in some cases, your tomograms may be too far out of the distribution of our training images. In this case, it can make sense to annotate several patches extracted from your tomogram, and re-train the model using your corrected data, together with our main training dataset.</p> <p>Our main training dataset is not publicly accessible yet, but if you would like to re-train MemBrain-seg, please contact us (Lorenz.Lamm@helmholtz-munich.de) and we will to find a solution.</p> <p>Here are some steps you can follow in order to re-train MemBrain-seg:</p>"},{"location":"Usage/Training/#step-1-prepare-your-training-dataset","title":"Step 1: Prepare your training dataset","text":"<p>MemBrain-seg assumes a specific data structure for creating the training dataloaders:</p> <pre><code>data_dir/\n\u251c\u2500\u2500 imagesTr/       # Directory containing training images\n\u2502   \u251c\u2500\u2500 img1.nii.gz    # Image file (currently requires nii.gz format)\n\u2502   \u251c\u2500\u2500 img2.nii.gz    # Image file\n\u2502   \u2514\u2500\u2500 ...\n\u251c\u2500\u2500 imagesVal/      # Directory containing validation images\n\u2502   \u251c\u2500\u2500 img3.nii.gz    # Image file\n\u2502   \u251c\u2500\u2500 img4.nii.gz    # Image file\n\u2502   \u2514\u2500\u2500 ...\n\u251c\u2500\u2500 labelsTr/       # Directory containing training labels\n\u2502   \u251c\u2500\u2500 img1.nii.gz  # Label file (currently requires nii.gz format)\n\u2502   \u251c\u2500\u2500 img2.nii.gz  # Label file\n\u2502   \u2514\u2500\u2500 ...\n\u2514\u2500\u2500 labelsVal/      # Directory containing validation labels\n    \u251c\u2500\u2500 img3.nii.gz  # Label file\n    \u251c\u2500\u2500 img4.nii.gz  # Label file\n    \u2514\u2500\u2500 ...\n</code></pre> <p>The data_dir argument is then passed to the training procedure (see Step 2).</p> <p>In addition to our main training dataset, you may want to add some corrected patches from your own tomograms to improve the network's performance on these.</p> <p>You can find some instructions here: How to create training annotations from your own tomogram?</p>"},{"location":"Usage/Training/#step-2-perform-training","title":"Step 2: Perform training","text":"<p>Performing the training is simple. After activating your virtual Python environment, you can type: <pre><code>membrain train\n</code></pre> to receive help with the input arguments. You will see that the only parameter you need to provide is the --data-dir argument:</p> <p><pre><code>membrain train --data-dir &lt;path-to-your-training-data&gt;\n</code></pre> This is exactly the folder you prepared in Step 1. </p> <p>Running this command should start the training and store the fully trained model in the ./checkpoint folder.</p> <p>Note: Training can take up to several days. We therefore recommend that you perform training on a device with a CUDA-enabled GPU.</p>"},{"location":"Usage/Training/#advanced-settings","title":"Advanced settings","text":"<p>In case you feel fancy and would like to adjust some of the default settings of MemBrain-seg, you can also use the following command to get access to more customizable options: ``` membrain train_advanced ```` This will display all options that can be activated / deactivated. For more in-depth adjustments, you will need to dig into MemBrain-seg's code or contact us.</p>"},{"location":"Usage/Training/#contact","title":"Contact","text":"<p>If there are any problems coming up when running the code or anything else is unclear, do not hesitate to contact us (Lorenz.Lamm@helmholtz-munich.de). We are more than happy to help.</p>"}]}